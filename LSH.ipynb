{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6803741f",
   "metadata": {},
   "source": [
    "# Tarea 1\n",
    "## IIC2440 - Procesamiento de Datos Masivos\n",
    "\n",
    "Integrantes: \n",
    "- Rodrigo Nahum\n",
    "- Fernando Quintana\n",
    "\n",
    "Notebook 1: LSH y obtención de tweets similares."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4125e04",
   "metadata": {},
   "source": [
    "# Importar datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdcd3cb5",
   "metadata": {},
   "source": [
    "Primero, importamos algunas librerías a usar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4894200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f39916c",
   "metadata": {},
   "source": [
    "Luego, importamos esta funcion que permite estimar la cantidad de memoria total que utiliza un objeto de Python. Obtenida de https://code.activestate.com/recipes/577504/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19bf9ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sys import getsizeof, stderr\n",
    "from itertools import chain\n",
    "from collections import deque\n",
    "try:\n",
    "    from reprlib import repr\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "def total_size(o, handlers={}, verbose=False):\n",
    "    \"\"\" Returns the approximate memory footprint an object and all of its contents.\n",
    "\n",
    "    Automatically finds the contents of the following builtin containers and\n",
    "    their subclasses:  tuple, list, deque, dict, set and frozenset.\n",
    "    To search other containers, add handlers to iterate over their contents:\n",
    "\n",
    "        handlers = {SomeContainerClass: iter,\n",
    "                    OtherContainerClass: OtherContainerClass.get_elements}\n",
    "\n",
    "    \"\"\"\n",
    "    dict_handler = lambda d: chain.from_iterable(d.items())\n",
    "    all_handlers = {tuple: iter,\n",
    "                    list: iter,\n",
    "                    deque: iter,\n",
    "                    dict: dict_handler,\n",
    "                    set: iter,\n",
    "                    frozenset: iter,\n",
    "                   }\n",
    "    all_handlers.update(handlers)     # user handlers take precedence\n",
    "    seen = set()                      # track which object id's have already been seen\n",
    "    default_size = getsizeof(0)       # estimate sizeof object without __sizeof__\n",
    "\n",
    "    def sizeof(o):\n",
    "        if id(o) in seen:       # do not double count the same object\n",
    "            return 0\n",
    "        seen.add(id(o))\n",
    "        s = getsizeof(o, default_size)\n",
    "\n",
    "        if verbose:\n",
    "            print(s, type(o), repr(o), file=stderr)\n",
    "\n",
    "        for typ, handler in all_handlers.items():\n",
    "            if isinstance(o, typ):\n",
    "                s += sum(map(sizeof, handler(o)))\n",
    "                break\n",
    "        return s\n",
    "\n",
    "    return sizeof(o)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82a942e0",
   "metadata": {},
   "source": [
    "Luego, importamos el dataset como un dataframe de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b08b9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('tweets_2022_abril_junio.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04717a52",
   "metadata": {},
   "source": [
    "Mostramos algunas filas para entender el formato del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a95da1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1512186166438637582</td>\n",
       "      <td>2022-04-07 21:50:51 UTC</td>\n",
       "      <td>h0l4d4ni3l4</td>\n",
       "      <td>RT @ValeMirandaCC: Tras casi 50 años del golpe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1512186202367045642</td>\n",
       "      <td>2022-04-07 21:51:00 UTC</td>\n",
       "      <td>Claudio70932894</td>\n",
       "      <td>RT @UTDTrabajoDigno: Mañana jueves a las 18hrs...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1512186287284924418</td>\n",
       "      <td>2022-04-07 21:51:20 UTC</td>\n",
       "      <td>Cesar_A_RR</td>\n",
       "      <td>RT @JaimeGuajardoR: Aquí está el aporte de @te...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1512186335754301446</td>\n",
       "      <td>2022-04-07 21:51:32 UTC</td>\n",
       "      <td>rosmarieher</td>\n",
       "      <td>RT @melnicksergio: la pelotudez no tiene limit...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1512186407841767424</td>\n",
       "      <td>2022-04-07 21:51:49 UTC</td>\n",
       "      <td>GQuelluen</td>\n",
       "      <td>RT @BSepulvedaHales: Ante la circulación de no...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id               created_at      screen_name   \n",
       "0  1512186166438637582  2022-04-07 21:50:51 UTC      h0l4d4ni3l4  \\\n",
       "1  1512186202367045642  2022-04-07 21:51:00 UTC  Claudio70932894   \n",
       "2  1512186287284924418  2022-04-07 21:51:20 UTC       Cesar_A_RR   \n",
       "3  1512186335754301446  2022-04-07 21:51:32 UTC      rosmarieher   \n",
       "4  1512186407841767424  2022-04-07 21:51:49 UTC        GQuelluen   \n",
       "\n",
       "                                                text  favorite_count   \n",
       "0  RT @ValeMirandaCC: Tras casi 50 años del golpe...               0  \\\n",
       "1  RT @UTDTrabajoDigno: Mañana jueves a las 18hrs...               0   \n",
       "2  RT @JaimeGuajardoR: Aquí está el aporte de @te...               0   \n",
       "3  RT @melnicksergio: la pelotudez no tiene limit...               0   \n",
       "4  RT @BSepulvedaHales: Ante la circulación de no...               0   \n",
       "\n",
       "   retweet_count  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5778d0d",
   "metadata": {},
   "source": [
    "# FIlter inicial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dcea8332",
   "metadata": {},
   "source": [
    "Partimos filtrando datos que no importan, como el created_at, o el favorite_count, retweet_count. Con esto, reducimos el tamaño del dataset en 77MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1988120",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_raw = data.filter(items=['id', 'screen_name', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13fd3f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = filtered_data_raw.astype({'screen_name': 'string', 'text': 'string'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82408158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                      int64\n",
       "screen_name    string[python]\n",
       "text           string[python]\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02958075",
   "metadata": {},
   "source": [
    "Tambien, vemos que existen algunos ids duplicados, por lo que tambien los removemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "670aeea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1512186166438637582</td>\n",
       "      <td>h0l4d4ni3l4</td>\n",
       "      <td>RT @ValeMirandaCC: Tras casi 50 años del golpe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1512186202367045642</td>\n",
       "      <td>Claudio70932894</td>\n",
       "      <td>RT @UTDTrabajoDigno: Mañana jueves a las 18hrs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1512186287284924418</td>\n",
       "      <td>Cesar_A_RR</td>\n",
       "      <td>RT @JaimeGuajardoR: Aquí está el aporte de @te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1512186335754301446</td>\n",
       "      <td>rosmarieher</td>\n",
       "      <td>RT @melnicksergio: la pelotudez no tiene limit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1512186407841767424</td>\n",
       "      <td>GQuelluen</td>\n",
       "      <td>RT @BSepulvedaHales: Ante la circulación de no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4594975</th>\n",
       "      <td>1526652300709679104</td>\n",
       "      <td>Alebarrera74</td>\n",
       "      <td>RT @DanielAbelLope1: @tere_marinovic 😡🤮😡🤮 VIEJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4594976</th>\n",
       "      <td>1526641118460334080</td>\n",
       "      <td>gigita29bq</td>\n",
       "      <td>RT @DanielAbelLope1: @tere_marinovic 😡🤮😡🤮 VIEJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4594977</th>\n",
       "      <td>1526738292011462657</td>\n",
       "      <td>Elizabe81480339</td>\n",
       "      <td>RT @Gonz1Gorjeperez: @tere_marinovic https://t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4594978</th>\n",
       "      <td>1526855280151056386</td>\n",
       "      <td>CastilloNafla</td>\n",
       "      <td>RT @Gonz1Gorjeperez: @tere_marinovic https://t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4594979</th>\n",
       "      <td>1526764015795310594</td>\n",
       "      <td>AndreaSakurada</td>\n",
       "      <td>RT @Gonz1Gorjeperez: @tere_marinovic https://t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4592806 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id      screen_name   \n",
       "0        1512186166438637582      h0l4d4ni3l4  \\\n",
       "1        1512186202367045642  Claudio70932894   \n",
       "2        1512186287284924418       Cesar_A_RR   \n",
       "3        1512186335754301446      rosmarieher   \n",
       "4        1512186407841767424        GQuelluen   \n",
       "...                      ...              ...   \n",
       "4594975  1526652300709679104     Alebarrera74   \n",
       "4594976  1526641118460334080       gigita29bq   \n",
       "4594977  1526738292011462657  Elizabe81480339   \n",
       "4594978  1526855280151056386    CastilloNafla   \n",
       "4594979  1526764015795310594   AndreaSakurada   \n",
       "\n",
       "                                                      text  \n",
       "0        RT @ValeMirandaCC: Tras casi 50 años del golpe...  \n",
       "1        RT @UTDTrabajoDigno: Mañana jueves a las 18hrs...  \n",
       "2        RT @JaimeGuajardoR: Aquí está el aporte de @te...  \n",
       "3        RT @melnicksergio: la pelotudez no tiene limit...  \n",
       "4        RT @BSepulvedaHales: Ante la circulación de no...  \n",
       "...                                                    ...  \n",
       "4594975  RT @DanielAbelLope1: @tere_marinovic 😡🤮😡🤮 VIEJ...  \n",
       "4594976  RT @DanielAbelLope1: @tere_marinovic 😡🤮😡🤮 VIEJ...  \n",
       "4594977  RT @Gonz1Gorjeperez: @tere_marinovic https://t...  \n",
       "4594978  RT @Gonz1Gorjeperez: @tere_marinovic https://t...  \n",
       "4594979  RT @Gonz1Gorjeperez: @tere_marinovic https://t...  \n",
       "\n",
       "[4592806 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.drop_duplicates(subset=['id'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8512ec0",
   "metadata": {},
   "source": [
    "Luego, guardamos este dataset para no tener que hacer el filtro de nuevo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d702572",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.to_csv('filtered_tweets_2022_abril_junio.csv', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1986524a",
   "metadata": {},
   "source": [
    "# Primer intento: Fuerza bruta.\n",
    "\n",
    "Intentamos hacer un doble for en el dataframe. Primero, vemos cuánto demora hacer una iteración completa por el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b2b35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item1 in filtered_data.itertuples():\n",
    "    pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ed0dc8c",
   "metadata": {},
   "source": [
    "Obtenemos que 1 iteracion tomaba como 3.2 seg. Entonces, el doble for iba a tomar la cantidad de filas * 3.2 seg, o sea, ~14.400.000 segundos ~ 166 dias. Obviamente no tenemos este tiempo. Como segundo intento, podemos pasar el dataframe a lista, porque estas son más rápidas para iterar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7d603d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_list = filtered_data.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97157494",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item1 in dataframe_list:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7b91d76",
   "metadata": {},
   "source": [
    "Esta vez, 1 iteración toma como 80ms. De nuevo, el doble for tomaría la cantidad de filas por 0.08s, o sea, 4 días. Si bien es menos, sigue siendo mucho tiempo. Entonces, intentamos algo más inteligente."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5813b935",
   "metadata": {},
   "source": [
    "# Segundo intento: Shingling y LSH"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84883499",
   "metadata": {},
   "source": [
    "Nuestra segunda estrategia corresponde a hacer Shingling de los textos de los tweets, y luego usar LSH para obtener tweets similares."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e156607",
   "metadata": {},
   "source": [
    "## Shingling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cf9ba54",
   "metadata": {},
   "source": [
    "Primero, cargamos el dataset ya filtrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f22365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = pd.read_csv('filtered_tweets_2022_abril_junio.csv', index_col=0, quotechar='\"')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4cfdb04",
   "metadata": {},
   "source": [
    "Luego, definimos una función para obtener los shingles de los textos, dado un k. Luego de probar distintos valores, decidimos usar k=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5e2d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_shingle(text, k=5):\n",
    "    shingles = set()\n",
    "    for idx in range(len(text) - k + 1):\n",
    "        curr_slice = text[idx : idx + k]\n",
    "        shingles.add(curr_slice)\n",
    "    return shingles\n",
    "\n",
    "k = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c55769eb",
   "metadata": {},
   "source": [
    "Una vez definido el k de los shingles, procedemos a borrar todos los tweets con menos de k caracteres, ya que a estos no le podemos obtener los shingles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34faa374",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = filtered_data[filtered_data['text'].apply(len) >= k]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d5ade09",
   "metadata": {},
   "source": [
    "Luego, nos damos cuenta que existen muchos tweets repetidos. Entonces, para evitar tratar con textos duplicados, guardamos una asociacion entre el texto, y el ID de los tweets que tienen ese texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf24cfa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1548053\n"
     ]
    }
   ],
   "source": [
    "tweets = {}\n",
    "\n",
    "for row in reduced_data.itertuples():\n",
    "    try:\n",
    "        tweets[row.text].append(row.id)\n",
    "    except KeyError:\n",
    "        tweets[row.text] = [row.id]\n",
    "\n",
    "print(len(tweets))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b95901b4",
   "metadata": {},
   "source": [
    "Esta tabla muestra la cantidad de tweets unicos para distintos k (el valor es distinto por el filtrado que hacemos, lo de remover los tweets de tamaño menor a k).\n",
    "\n",
    "| k | tweets  | \n",
    "|---|---------|\n",
    "| 5 | 1548053 |\n",
    "| 6 | 1547594 |\n",
    "| 7 | 1547038 |\n",
    "| 8 | 1546393 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33e98a47",
   "metadata": {},
   "source": [
    "Vemos que existen solo ~1500000 de tweets unicos, o sea, un tercio de los originales. Entonces, tenemos que procesar un tercio de los documentos. Luego, usamos una lista para definir un orden inicial (arbitrario) de los tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18af2be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tweets = list(tweets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dc5aa600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"unique_tweets_k={k}.pkl\", 'wb') as f:\n",
    "    pickle.dump(unique_tweets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cccaccc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316745930"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_size(unique_tweets)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21aa7dca",
   "metadata": {},
   "source": [
    "Vemos que la cantidad de memoria que utiliza el texto de todos los tweets es de 316MB, un tamaño perfectamente razonable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f3a2c97",
   "metadata": {},
   "source": [
    "### Primera forma: Obtener todos los Shingles y mantenerlos en memoria."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7111b3d0",
   "metadata": {},
   "source": [
    "Intentamos computar todos los shingles al inicio, y luego usarlos para hacer LSH más rápido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f76d84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 100000\n"
     ]
    }
   ],
   "source": [
    "tweet_shingles = {}\n",
    "\n",
    "i = 1\n",
    "for row in filtered_data.itertuples():\n",
    "    if i % 100000 == 0:\n",
    "        print(f\"i: {i}\")\n",
    "        ## Ponemos un break para solo computar 100000, y no\n",
    "        ## todos los shingles, porque esto no se puede.\n",
    "        break\n",
    "    tweet_shingles[row.id] = k_shingle(row.text, k)\n",
    "    i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d91ecbda",
   "metadata": {},
   "source": [
    "Primero, obtenemos 100000 conjuntos de shingles, para ver el uso de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e4d33db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1252849614"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_size(tweet_shingles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b20abef7",
   "metadata": {},
   "source": [
    "Vemos que tener 100000 shingles usa 1.25GB de memoria. Entonces, computar los ~1500000 conjuntos de shingles utilizaría 18.75GB de memoria. Este valor ya es muy alto, asi que decidimos buscar otra opción."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa28c0a1",
   "metadata": {},
   "source": [
    "### Segunda forma: Obtener los shingles de los textos on-the-fly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff1b9a05",
   "metadata": {},
   "source": [
    "Para solucionar el problema anterior, calculamos primero todos los shingles existentes (o sea, todos los shingles presentes en algun documento). Notemos que esto usa mucha menos memoria que lo anterior, que computaba un conjunto por cada shingle, y podíamos tener shingles repetidos. Ahora, no tenemos shingles repetidos. Luego, generamos algún ordenamiento (permutación) de estos shingles. Después, para cada texto, computamos sus shingles, vemos el shingle que antes aparece segun esa permutacion, y ese es el valor de LSH(text). Entonces, guardamos solamente {bucket: text_id}, y no tenemos que guardar todos los shingles simultaneamente en memoria."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f294e38",
   "metadata": {},
   "source": [
    "Entonces, primero computamos todos los shingles existentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "abab9e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "i: 100000\n",
      "i: 200000\n",
      "i: 300000\n",
      "i: 400000\n",
      "i: 500000\n",
      "i: 600000\n",
      "i: 700000\n",
      "i: 800000\n",
      "i: 900000\n",
      "i: 1000000\n",
      "i: 1100000\n",
      "i: 1200000\n",
      "i: 1300000\n",
      "i: 1400000\n",
      "i: 1500000\n"
     ]
    }
   ],
   "source": [
    "all_shingles = set()\n",
    "\n",
    "i = 0\n",
    "for row in tweets.keys():\n",
    "    if i % 100000 == 0:\n",
    "        print(f\"i: {i}\")\n",
    "    current_shingles = k_shingle(row, k)\n",
    "    all_shingles.update(current_shingles)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c02c6df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shingles amount: 4139037\n",
      "Total size     : 381308486\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shingles amount: {len(all_shingles)}\")\n",
    "print(f\"Total size     : {total_size(all_shingles)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2579a621",
   "metadata": {},
   "source": [
    "Vemos que tenemos 4.139.037 shingles en total, y que guardar todos estos usa solamente 381MB de RAM. De nuevo, es un valor razonable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec8af250",
   "metadata": {},
   "source": [
    "Luego, usamos una lista para definir un orden inicial (arbitrario) de los shingles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf62e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "shingles_list = list(all_shingles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33685f81",
   "metadata": {},
   "source": [
    "Luego, creamos un diccionario que, dado un shingle, retorne su posición en el ordenamiento inicial. De esta forma, podemos saber la posición de un shingle en O(1), y no en O(log(n)) usando, por ejemplo, búsqueda binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d458c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shingles_dict = {shingle: idx for idx, shingle in enumerate(shingles_list)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f0ca186",
   "metadata": {},
   "source": [
    "Ahora que tenemos todos los shingles, definimos funciones para computar el LSH de cada texto. Primero, definimos una forma de obtener funciones de hash, para simular una permutación de la lista original.\n",
    "\n",
    "Luego, definimos una función para obtener el LSH de los textos. Para esto, primero definimos una cantidad de funciones de hash. Luego, para cada función de hash (permutación), obtenemos la posición de cada shingle según esta permutación, y nos quedamos con el mínimo. Hacemos esto para cada función de hash, y el vector resultante será el LSH del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "887a1dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forma de definir funciones de hash adaptada de la actividad 6 del curso\n",
    "## (https://github.com/IIC2440/Syllabus-2023-1/blob/main/Actividades/06%20-%20LocallySensitiveHashing/Shingling_jaccard_universalhashes.ipynb)\n",
    "def create_hash(n):\n",
    "    ## Primo de 9 digitos elegido al azar con https://bigprimes.org/\n",
    "    p = 791639819\n",
    "    a = random.randint(1, p - 1)\n",
    "    b = random.randint(1, p - 1)\n",
    "    return lambda x: ((a * x + b) % p) % n\n",
    "\n",
    "## Funcion basada en hashing en vez de generar una permutacion.\n",
    "## Puede tener colisiones.\n",
    "def compute_LSH_signatures(data, hash_funcs=10):\n",
    "    ## Primero, obtenemos la cantidad de funciones de hash pedida.\n",
    "    hashes = [create_hash(len(data)) for _ in range(hash_funcs)]\n",
    "\n",
    "    ## Luego, inicializamos nuestra matriz de LSH. Cada fila corresponde\n",
    "    ## a una funcion de hash, y cada columna corresponde a un texto.\n",
    "    ## Entonces, una columna tendrá el vector correspondiente al valor\n",
    "    ## de cada funcion de hash para ese texto.\n",
    "    LSH_signature = np.zeros((hash_funcs, len(data)), dtype=np.int32)\n",
    "\n",
    "    i = 0\n",
    "    ## Para cada texto\n",
    "    for row in tqdm(data):\n",
    "\n",
    "        ## Obtenemos el conjunto de shingles de ese texto.\n",
    "        row_shingles = list(k_shingle(row, k))\n",
    "        \n",
    "        ## Luego, obtenemos las posiciones de estos shingles en el ordenamiento\n",
    "        ## original.\n",
    "        row_shingles_idx = [shingles_dict[shingle] for shingle in row_shingles]\n",
    "\n",
    "        ## Para cada funcion de hash (permutacion), obtenemos la posicion de cada\n",
    "        ## shingle en la permutacion, y nos quedamos con el minimo (o sea, el primer\n",
    "        ## shingle del texto segun la permutacion).\n",
    "        ## Notamos que esta linea calcula el vector completo.\n",
    "        row_lsh = [min(map(hash_func, row_shingles_idx)) for hash_func in hashes]\n",
    "\n",
    "        ## Actualizamos nuestra matriz con este vector.\n",
    "        LSH_signature[:, i] = row_lsh\n",
    "        i += 1\n",
    "    \n",
    "    return LSH_signature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46f07425",
   "metadata": {},
   "source": [
    "Luego, usamos 200 funciones de hash, y computamos el LSH de los textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f487ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_funcs = 200\n",
    "\n",
    "hash_lsh_signature = compute_LSH_signatures(unique_tweets, hash_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "14388f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f\"hash_signatures_k={k}_hash_funcs={hash_funcs}.npy\", hash_lsh_signature)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a86849b",
   "metadata": {},
   "source": [
    "## Computo de tweets similares"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "733f7a52",
   "metadata": {},
   "source": [
    "Una vez que computamos las signatures, procedemos a obtener pares de tweets similares. Para esto, primero definimos b y r, tal que b * r ~ 200 (porque usamos 200 funciones de hash). Luego, diremos que 2 tweets $t_1$ y $t_2$ son similares, si para alguna banda *b*, los *r* hashes de esa banda de $t_1$ y $t_2$ son iguales. Notemos que esta definición nos permite procesar cada banda por separado, o sea, a una banda posterior no le importa el resultado de las bandas anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "89c70c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:33<00:00,  4.68s/it]\n"
     ]
    }
   ],
   "source": [
    "r = 10\n",
    "b = 20\n",
    "\n",
    "## Set de tweets similares. Cada elemento es una tupla\n",
    "## (t1, t2), donde t1 y t2 son tweets similares.\n",
    "similar_items = set()\n",
    "\n",
    "## Para cada banda\n",
    "for band in tqdm(range(b)):\n",
    "    band_hash = {}\n",
    "    ## Para cada tweet\n",
    "    for tweet in range(hash_lsh_signature.shape[1]):\n",
    "        ## Obtenemos el vector asociado a ese tweet\n",
    "        tweet_vector = tuple(hash_lsh_signature[band * r:(band + 1)*r, tweet])\n",
    "        ## Lo agregamos a una lista que contiene todos los tweets con ese mismo\n",
    "        ## vector\n",
    "        try:\n",
    "            band_hash[tweet_vector].append(tweet)\n",
    "        except KeyError:\n",
    "            band_hash[tweet_vector] = [tweet]\n",
    "    ## Luego de obtener todas las listas similares, reportamos cada par en\n",
    "    ## una misma lista como similar.\n",
    "    for key in band_hash:\n",
    "        for item1 in band_hash[key]:\n",
    "            for item2 in band_hash[key]:\n",
    "                ## Agregamos items similares a nuestro set de \n",
    "                ## tweets similares.\n",
    "                if item1 != item2:\n",
    "                    ## Agregamos el elemento mas grande como primer\n",
    "                    ## elemento de la tupla siempre, para evitar\n",
    "                    ## duplicados. Si (a, b) es una tupla similar, \n",
    "                    ## entonces (b, a) no lo sera.\n",
    "                    if item1 > item2:\n",
    "                        similar_items.add((item1, item2))\n",
    "                    else:\n",
    "                        similar_items.add((item2, item1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8bb8df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"similar_items_k={k}_hash_funcs={hash_funcs}_b={b}_r={r}.pkl\", 'wb') as f:\n",
    "    pickle.dump(similar_items, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "04f229df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4716440"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(similar_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28d19538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134217944"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getsizeof(similar_items)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d2c1959",
   "metadata": {},
   "source": [
    "De esta forma, obtenemos 4.716.440 pares de tweets similares. Si bien esto suena como mucho, posteriormente, cuando encontremos personas que escriben similar, este número se reducirá mucho. Además, los items similares pesan 134MB, un tamaño razonable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
